# 線性代數與幾何變換整理

## 1. 核心概念：線性、代數與空間

### 什麼是「線性」？
- **線性 (Linear)** 滿足兩個基本特性：
  1. **可加性**：  
     f(x + y) = f(x) + f(y)

  2. **齊次性**：  
  
     f(ax) = a f(x)
     
- 幾何意義：
  - 直線經過線性變換後依然是直線
  - 原點位置保持不變

### 什麼是「代數」？
- 來源：阿拉伯語 **"al-jabr"**，意指「破碎部分的結合」  
- 在數學中：
  - 使用符號代表數值
  - 透過加減乘除等運算規則解方程式

### 線性代數
- 研究「線性映射」與「線性方程式組」的代數系統

### 什麼是「空間」與「向量空間」？
- **空間 (Space)**：一個集合加上一套規則（結構）
- **向量空間 (Vector Space)**：
  - 允許向量在其中進行 **加法** 與 **純量乘法**
  - 結果仍留在集合內
  - 提供一個可以「移動」與「縮放」的完備環境

---

## 2. 矩陣與幾何變換

### 矩陣與向量的關係
- 矩陣可以視為一個函數或算子  
- 當矩陣 A 乘以向量 x：

  A x = y
  
  代表 x  被從一個位置「變換」到另一個位置y

### 幾何操作的矩陣表示

#### 縮放 (Scaling)
- 對角矩陣表示：
  $$
  \text{2D: } 
  \begin{bmatrix} s_x & 0 \\ 0 & s_y \end{bmatrix}
  $$

#### 旋轉 (Rotation)
- 2D 旋轉 $\theta$ 弧度：
  $$
  \begin{bmatrix} 
  \cos\theta & -\sin\theta \\ 
  \sin\theta & \cos\theta 
  \end{bmatrix}
  $$

#### 平移 (Translation)
- 平移不滿足線性，需要使用 **齊次座標**：
  $$
  \begin{bmatrix} 
  1 & 0 & t_x \\ 
  0 & 1 & t_y \\ 
  0 & 0 & 1 
  \end{bmatrix} 
  \begin{bmatrix} x \\ y \\ 1 \end{bmatrix} 
  = 
  \begin{bmatrix} x + t_x \\ y + t_y \\ 1 \end{bmatrix}
  $$

---

## 3. 行列式 (Determinant) 的意義與幾何連結

- **行列式** $\det(\mathbf{A})$ 表示變換後的空間縮放比例
  - $\det(\mathbf{A}) = 2$ → 面積或體積變成原來的 2 倍
  - $\det(\mathbf{A}) = 0$ → 空間壓縮到更低維度（矩陣不可逆）

### 計算方式
1. **遞迴公式 (Laplace Expansion)**
   - 將 $n \times n$ 矩陣降階為 $(n-1) \times (n-1)$
2. **對角化快速計算**
   - 若 $\mathbf{A} = \mathbf{PDP}^{-1}$  
     $$
     \det(\mathbf{A}) = \det(\mathbf{D}) = \lambda_1 \lambda_2 \dots \lambda_n
     $$
   - $\lambda_i$ 為特徵值
3. **LU 分解快速計算**
   - $\mathbf{A} = \mathbf{LU}$  
   - $L$ 為單位下三角矩陣 → $\det(L) = 1$  
   - $U$ 為上三角矩陣 → $\det(\mathbf{A}) = \prod U_{ii}$

---

## 4. 特徵值與矩陣分解

### 特徵值 (Eigenvalue) 與 特徵向量 (Eigenvector)
- 當矩陣 $\mathbf{A}$ 作用於向量 $\mathbf{v}$：
  $$
  \mathbf{A} \mathbf{v} = \lambda \mathbf{v}
  $$
  - $\mathbf{v}$ 的方向不變，長度縮放 $\lambda$ 倍
  - $\mathbf{v}$ → **特徵向量**  
    $\lambda$ → **特徵值**

### 常見分解與用途
1. **QR 分解**
   - $\mathbf{A} = \mathbf{Q} \mathbf{R}$
   - $\mathbf{Q}$ 正交矩陣，$\mathbf{R}$ 上三角矩陣
   - 用途：最小平方法、特徵值計算

2. **SVD (奇異值分解)**
   - $\mathbf{A} = \mathbf{U \Sigma V}^T$
   - 適用於任意矩陣（不限方陣）
   - $\mathbf{U}, \mathbf{V}$ → 旋轉  
     $\mathbf{\Sigma}$ → 縮放

3. **PCA (主成分分析)**
   - 使用 SVD 或特徵值分解
   - 找出數據變異最大的方向
   - 用於數據降維
